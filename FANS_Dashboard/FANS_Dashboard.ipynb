{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FANS Dashboard\n",
    "This Jupyter Notebook is designed to post-process, interpret, and visualize results generated by FANS. The notebook provides tools for exploring the hierarchical structure of HDF5 files, extracting and summarizing simulation data, and preparing the results for visualization in ParaView.\n",
    "\n",
    "## Index of Contents\n",
    "\n",
    "1. [Prerequisites](#Prerequisites)\n",
    "2. [Importing Required Modules](#importing-required-modules)\n",
    "3. [Load and Explore Simulation Results](#load-and-explore-simulation-results)\n",
    "4. [Specify and Extract Data for Analysis](#specify-and-extract-data-for-analysis)\n",
    "5. [Compute Tensor Measures](#compute-tensor-measures)\n",
    "6. [Post-processing and Writing Data to HDF5](#post-processing-and-writing-data-to-hdf5)\n",
    "7. [Exporting Data to XDMF for Visualization](#Exporting-data-to-XDMF-for-Visualization)\n",
    "7. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "To start using the FANS Dashboard, ensure that the necessary Python packages and modules are installed. The required imports in your Jupyter Notebook should include:\n",
    "- `numpy`\n",
    "- `h5py`\n",
    "- `plotly`\n",
    "- `collections`\n",
    "- `argparse`\n",
    "- `lxml`\n",
    "- `re`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Modules\n",
    "These modules provide essential functions for data extraction, post-processing, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from FANS_Dashboard_utils import *\n",
    "from postprocessing import compute_rank2tensor_measures\n",
    "from h52xdmf import write_xdmf\n",
    "from plotting import plot_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Explore Simulation Results\n",
    "To begin working with your FANS simulation results, first load the data from an HDF5 file. You can specify the path to your results file via `file_path`. \n",
    "\n",
    "Before diving into analysis or visualization, it’s crucial to understand the structure of your data. This can be done using the `identify_hierarchy` function. This function traverses the HDF5 file and returns a dictionary that represents the hierarchical structure of the data, including microstructures, load cases, and time steps. Each dataset's name and shape are captured, giving you a detailed map of the contents of the file.\n",
    "\n",
    "After identifying the structure, you can summarize it to get a quick overview of what’s inside your HDF5 file using `summarize_hierarchy`. This function prints a summary to the console, including the number of microstructures, load cases, time steps, and available quantities for each load case. \n",
    "<!-- For example, you might see output like this:\n",
    "```\n",
    "Found 1 microstructure(s):\n",
    "  Microstructure '/sphere/32x32x32/ms' has 1 load case(s):\n",
    "    Load case 'load0' has 51 time step(s) and the following quantities:\n",
    "      - absolute_error with shape (1,)\n",
    "      - displacement with shape (32, 32, 32, 3)\n",
    "      - microstructure with shape (32, 32, 32, 1)\n",
    "      - strain with shape (32, 32, 32, 6)\n",
    "      - strain_average with shape (6,)\n",
    "      - stress with shape (32, 32, 32, 6)\n",
    "      - stress_average with shape (6,)\n",
    "``` -->\n",
    "\n",
    "This summary helps you quickly understand the dataset without manually exploring the file, making it easier to decide which data to analyze or visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "file_path = '../test/test_results.h5'\n",
    "\n",
    "# Extract hierarchy information from the file\n",
    "hierarchy = identify_hierarchy(file_path)\n",
    "summarize_hierarchy(hierarchy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Specify and Extract Data for Analysis\n",
    "After exploring and summarizing the hierarchy of your simulation data, the next step is to specify which parts of the data you want to load for further analysis or visualization. This step involves selecting specific microstructures, load cases, quantities, and time steps.\n",
    "This allows to narrow down the data you want to work with. Instead of loading the entire dataset, which could be large and cumbersome, you can focus on specific microstructures, load cases, quantities, and time steps that are relevant to your analysis.\n",
    "\n",
    "The `extract_and_organize_data` function is used to extract the specified data from the HDF5 file and organize it into a structured format for further processing. Here’s what it does:\n",
    "\n",
    "- Hierarchical Navigation: The function navigates through the hierarchical structure of the HDF5 file, which has already been identified by the `identify_hierarchy` function.\n",
    "\n",
    "- Selection Criteria: It filters the data based on the user-specified microstructures, load cases, quantities, and time steps. Only the relevant data is extracted.\n",
    "\n",
    "- Data Organization: The extracted data is organized into a nested dictionary structure. This structure is indexed by microstructure, load case, and quantity, with each dataset being stored as a NumPy array.\n",
    "\n",
    "- Aggregation by Time Steps: The function also handles the time dimension, stacking data across different time steps along the 0th axis of the `NumPy` arrays. This makes it easier to process time-series data in subsequent steps.\n",
    "\n",
    "- Returning Organized Data: Finally, the organized data is returned as a dictionary, where the keys correspond to microstructures, load cases, and quantities, and the values are the extracted data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which microstructures, load cases, and quantities to load\n",
    "microstructures_to_load = ['/sphere/32x32x32/ms']\n",
    "load_cases_to_load = ['load0']\n",
    "quantities_to_load = ['strain_average', 'stress_average', 'phase_stress_average_phase0', \"stress\"]\n",
    "time_steps_to_load = []\n",
    "\n",
    "# Extract the specified data, organized and sorted by time steps\n",
    "data = extract_and_organize_data(file_path, hierarchy, \n",
    "                                 quantities_to_load, \n",
    "                                 microstructures_to_load, \n",
    "                                 load_cases_to_load, \n",
    "                                 time_steps_to_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compute Tensor Measures\n",
    "After selecting and organizing the data, various measures from the tensor data (e.g., stress or strain tensors) can be computed via `compute_rank2tensor_measures` function. This derives important scalar or vector measures of these Rank-2 tensors that may be crucial for interpreting the physical state of the simulated system.\n",
    "\n",
    "- Input Data: The input to the function are symmetric Rank-2 tensors in Mandel notation ([e11, e22, e33, e12, e13, e23]), where the last dimension corresponds to the six independent components of the tensor.\n",
    "\n",
    "- Output Data: The output is a dictionary where each entry corresponds to one of the computed measures, with the data organized in NumPy arrays that match the shape of the input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_average = data['/sphere/32x32x32/ms']['load0']['strain_average']\n",
    "stress_average = data['/sphere/32x32x32/ms']['load0']['stress_average']\n",
    "time_steps = data['/sphere/32x32x32/ms']['load0']['time_steps']\n",
    "\n",
    "# Specify measures to compute\n",
    "measures_to_compute = ['von_mises', 'hydrostatic', 'deviatoric', 'principal', \n",
    "                       'max_shear', 'I_invariants', 'J_invariants', 'eigenvalues',\n",
    "                       'eigenvectors', 'lode_angle']\n",
    "\n",
    "# Compute the requested measures\n",
    "results = compute_rank2tensor_measures(stress_average, measures_to_compute)\n",
    "\n",
    "stress_average_deviatoric = results['deviatoric']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Post-processing and Writing Data to HDF5\n",
    "\n",
    "The measures can be post-processed and written back to the HDF5 file via `postprocess_and_write_to_h5`. This allows to extend the original dataset with new, derived quantities that can be used for further analysis or visualization.\n",
    "\n",
    "The `postprocess_and_write_to_h5` function handles the entire workflow of extracting data, processing it to compute the desired measures, and then writing the results back into the HDF5 file. Here's how it works:\n",
    "\n",
    "- Data Extraction: The function first uses `extract_and_organize_data` to load the relevant data from the HDF5 file.\n",
    "\n",
    "- Processing the Data: Once the data is extracted, the function computes the specified measures using the `compute_rank2tensor_measures` function.\n",
    "\n",
    "- Naming Convention: The processed measures are saved under a new naming convention in the format quantity_measure (e.g., stress_average_von_mises). This makes it clear which original quantity each measure was derived from.\n",
    "\n",
    "- Writing to HDF5: The function then writes the processed data back into the HDF5 file at the appropriate locations. The `write_processed_data_to_h5` helper function ensures that the data is stored correctly, using the appropriate group and dataset paths within the HDF5 file.\n",
    "\n",
    "- Return Value: The function returns a dictionary containing the processed data, which can be used for further in-memory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocessing and writing to h5 file\n",
    "quantities_to_postprocess = ['stress_average', 'stress']\n",
    "measures_to_postprocess = ['deviatoric', 'von_mises']\n",
    "microstructures_to_postprocess = ['/sphere/32x32x32/ms']\n",
    "load_cases_to_postprocess = ['load0']\n",
    "\n",
    "processed_data = postprocess_and_write_to_h5(file_path, hierarchy, quantities_to_postprocess, measures_to_postprocess, \n",
    "                                             microstructures_to_postprocess, load_cases_to_postprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualizing basic data\n",
    "\n",
    "After processing the data and computing the desired tensor measures, some basic measures can be visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot stress average vs strain average for each component\n",
    "\n",
    "plot_subplots(strain_average, stress_average, labels_x=['Strain']*6, labels_y=['Stress']*6, \n",
    "              subplot_titles=['component - 11', 'component - 22', 'component - 33', 'component - 12', 'component - 13', 'component - 23'], \n",
    "              title='Stress average vs strain average', \n",
    "              nrows=2, ncols=3, linewidth=1, markersize=4, linecolor='blue', markercolor='blue', aspect_ratio=(1,1), fontsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot deviatoric stress average vs strain average for each component\n",
    "plot_subplots(strain_average, stress_average_deviatoric, labels_x=['Strain']*6, labels_y=['Stress_dev']*6,\n",
    "                subplot_titles=['component - 11', 'component - 22', 'component - 33', 'component - 12', 'component - 13', 'component - 23'],\n",
    "                title='Deviatoric stress average vs strain average',\n",
    "                nrows=2, ncols=3, linewidth=1, markersize=4, linecolor='blue', markercolor='blue', aspect_ratio=(1,1), fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot deviatoric stress average vs time\n",
    "time_broadcasted = np.tile(time_steps[:, np.newaxis], (1, stress_average.shape[1]))\n",
    "plot_subplots(time_broadcasted, stress_average_deviatoric, labels_x=['Time']*6, labels_y=['Stress_dev']*6,\n",
    "                subplot_titles=['component - 11', 'component - 22', 'component - 33', 'component - 12', 'component - 13', 'component - 23'],\n",
    "                title='Deviatoric stress average vs time',\n",
    "                nrows=2, ncols=3, linewidth=1, markersize=4, linecolor='blue', markercolor='blue', aspect_ratio=(1,1), fontsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subplots(\n",
    "    np.column_stack((strain_average[:, 0], strain_average[:, 0])), \n",
    "    np.column_stack((stress_average[:, 0], stress_average_deviatoric[:, 0])), \n",
    "    labels_x=['Strain_11']*2, labels_y=['Stress_11', 'Stress_dev_11'],\n",
    "    subplot_titles=['Stress_11 vs Strain_11', 'Stress_dev_11 vs Strain_11'],\n",
    "    nrows=1, ncols=2, linewidth=1, markersize=4, linecolor='blue', markercolor='blue', aspect_ratio=(1,1), fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Exporting Data to XDMF for Visualization\n",
    "The final step in the workflow is to export the HDF5 data to an [XDMF](https://www.xdmf.org/index.php/XDMF_Model_and_Format) file, which is a format that can be easily read by visualization tools like [ParaView](https://www.paraview.org/). This is particularly useful when you want to visualize the spatial and temporal evolution of fields such as stress, strain, or any other tensor quantities in 3D. \n",
    "\n",
    "The `write_xdmf` function converts the data stored in an HDF5 file into an XDMF format. This allows you to map the structured data in the HDF5 file to a grid-based format that can be rendered in 3D, providing insights into the distribution and evolution of physical quantities across the microstructure over time.\n",
    "\n",
    "In addition this, the `h52xdmf.py` script can also be used as a command-line tool. This allows for flexible conversion of HDF5 files to XDMF format. For further details run `python h52xdmf.py --help`. This will display detailed usage information, including all available options and arguments.\n",
    "\n",
    "\n",
    "### Visualization in ParaView\n",
    "Once the XDMF file is generated, it can be loaded into ParaView. ParaView will recognize the spatial and temporal structure of the data, allowing you to explore the 3D microstructure interactively. You can create slices, isosurfaces, and animations to visualize how quantities like stress and strain evolve over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_xdmf(h5_filepath=file_path, xdmf_filepath='test_results.xdmf',\n",
    "           microstructure_length=[1,1,1], time_series=True, time_keyword='time_step', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The FANS Dashboard provides a streamlined workflow for interpreting and visualizing FANS simulation results. By following the steps outlined above, you can effectively explore your data, perform critical post-processing, and prepare your results for detailed visualization in ParaView.\n",
    "\n",
    "For more advanced usage and customizations, explore the full set of utilities available in the FANS Dashboard modules.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
